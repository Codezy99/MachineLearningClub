---
title: >
  ISLR - Chapter 9 Labs
author: Lars Hadidi und Manuel Herbst
output: 
  html_document:
  # BiocStyle::html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: cosmo
    code_folding: show
    code_download: true
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  message = FALSE,
  echo = TRUE,
  warnings = FALSE,
  fig.align = "center"
)
```

# Support Vector Classifier

# Support Vector Machine
We use an SVM if we want to have a "curved"/ "nonlinear" decision boundary for our classification:  

```{r echo=FALSE, out.width="30%"}
knitr::include_graphics(path = "SVM_files/fig 17.png")
```
The "nonlinear" decision boundary curve is given by the zeros of a "nonlinear" function of the form $$f(x) = -\rho + \alpha_1K(x_1,x) + \alpha_2K(x_2,x) + \ldots + \alpha_nK(x_n,x)$$

The kernel map $K$ is either the linear, the polynomial or the radial Kernel.

The predicted class label of an observation $x$ is based on whether $f(x) \geq 0$ or $f(x) < 0$. The value $f(x)$ is referred to as **fitted value** for the observation $x$.

The **margins** are given by the level sets $f = 1$ and by $f = -1$. 

A **support vector** is an observation $x_i$ from class $y_i\in \pm 1$ that violates the margin in the sense that $y_if(x_i) \leq 1$. This includes observations on the margin.


For this LAB, We need the following libraries:

* Want to use the **svm()** function:
```{r}
library(e1071)
```
* Want to plot ROC curves:
```{r}
library(ROCR)
```
* Want to use the Gene Expression Data set **Khan**
```{r}
library(ISLR)
```


## Data simulation
1. Generate some data with a non-linear class boundary and plot it
```{r}
# In order to have reproducable simulated data
set.seed(1)

# We want to have n = 200 observations with p = 2 real-valued normally distributed features
# The first 150 observations shall belong to class 1, the remaining 50 observations shall belong to class 2
y <- c(rep(1, 150), rep(2, 50))
x <- matrix(rnorm(200 * 2), ncol = 2)

# We move the class 1 observations in order to
# distinguish them from class 2 observations
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2

# Convert simulated data in a data frame:
dat <- data.frame(x = x, y = as.factor(y)) 
# Do not forget as.factor()!!
# Class labels need to be a factor variable
# Otherwise: svm()-function performs Support Vector Regression

# Class labels 1 and 2 because of the color:
# 1 = black
# 2 = red
plot(x[,2], x[,1], col = y) 
```
2. We randomly choose 100 training observations (training data set). The remaining 100 observations build our test training data set
```{r}
train <- sample(200, 100) # choose 100 out of 200 without replacement

# Training data set
data.train <- dat[train,]

# Test data set
data.test <- dat[-train,]
# The Minus sign reads: leave out all training observations

# Plot the training data:
plot(data.train[,2], data.train[,1] , col = data.train[,3])
```

## Training a SVM using svm()
```{r}
library(e1071)

#?svm()
```
Input arguments (only the most important):

1. **x** and/or **data**: matrix or data frame containing the observed features
2. **y** : vector containing the response values; in our setting:     containing the class labels **as factors!!!**

3. **kernel**: specifying the kernel 
  + Linear Kernel: **kernel = "linear"**,  
$$
K(x,y) := \left\langle x,y \right\rangle
$$
  
  + Polynomial kernel: **kernel = "polynomial"**, 
$$
K(x,y) := (c_0+\gamma\left\langle x,y \right\rangle)^d
$$  
   also specifying **degree**, **coef0** and **gamma**; 
   default: $d=3$, $c_0 = 0$, $\gamma = (\text{data dimension})^{-1}$
  + Radial kernel (RBF): **kernel = "radial"**, 
$$
K(x,y) := \exp(-\gamma \left\|x-y\right\|^2)
$$
  
  specifying tuning parameter **gamma** $\gamma$.
  
  default: "radial"

4. **cost**: Parameter regulating the Bias-Variance-Tradeoff
   + Large cost parameter heavily penalize violations to the margins, result: only few support vectors, narrow margins, very sensitive reaction to outliers (Low-Bias-High-Variance estimator)
   + Small cost parameters: many support vectors, wide margins, recognize outliers (High-Bias-Low-Variance estimator)
   + default: cost = 1
5. **scale = TRUE**: Scale each feature to have mean zero and unit variance 
6. $\textbf{decision.values = TRUE}$: If you need the fitted values $f(x)$ for observations $x$. 
 
Output of svm(): An object of class "svm" containing the fitted model 


```{r}

# Fit a model unsing a radial kernel with gamma = 1 and cost = 1
svm.out <- svm(y ~ ., data = data.train, kernel = "radial", gamma = 1, cost = 1, decision.values = T)

# To obtain some information about the fitted model:
typeof(svm.out)
class(svm.out)
summary(svm.out)
```
Want to visualize the model:


```{r}
svm.plot <- function(svm.out, data.train){
  # First plot: Decision areas, Support Vectors are represented by a cross
  plot(svm.out, data.train)
  
  # Second plot: Decision boundary and margins added to the data
  plot(data.train[,c(2,1)], col = data.train[,3])
  
  # Generating grid points:
  min <- min(data.train[,c(2,1)])
  max <- max(data.train[,c(2,1)])
  x2 <- seq(from = min -1, to = max+1, by = 0.1)
  x1 <- x2
  xgrid <- data.frame(expand.grid(X1=x1,X2=x2)) 
  names(xgrid) <- names(data.train)[c(2,1)]
  
  # For each point x of the grid, we determine the fitted value f(x)
  # This enables us to determine the contour lines of f=0, f=1 and f=-1 respectively
  # Note: Applying the function predict() yields only the predicted class labels. On has to set decision.values = TRUE and the fitted values can then be obtained via the attribute() function:
fit <- attributes(predict(svm.out,xgrid, decision.values = T))$decision.values 
  f <- matrix(fit,ncol = length(x1))
  # Now we can add the decision boundary and the margins to the data
  contour(x2,x1,f,add=T, levels = 0, lty = 1)
  contour(x2,x1,f,add=T, levels = 1, lty = 2)
  contour(x2,x1,f,add=T, levels = -1, lty = 2)
  #return()
}
# We call the function svm.plot():
svm.plot(svm.out,data.train)
```
In our example, there were `r length(svm.out$index)` support vectors.

Claim: SVM algorithm produces the same decision boundary, using only support vectors. 

```{r}
# Which observation is a support vector?
# The support vector indices can be obtained via svm.out$index:
data.sv.only <- data.train[svm.out$index, ]

# SVM using support vectors only
svm.sv.only <- svm(y ~ ., data = data.sv.only, kernel = "radial", gamma = 1, cost = 1, decision.values = T)
svm.plot(svm.sv.only,data.sv.only)
```
Claim: Increasing the **cost** parameter reduces the number of support vectors and margin width at the price of a more irregular decision boundary (risk of overfitting the data):
```{r}
svm.overfit <- svm(y ~ ., data = data.train, kernel = "radial", gamma = 1, cost = 1e5, decision.values = T)
svm.plot(svm.overfit, data.train)
```
With a cost parameter of $10^5$, there were `r length(svm.overfit$index)` support vectors.

Claim: Increasing the **gamma** parameter yields a more irregular decision boundary with a "local" behaviour (risk of overfitting the data)
```{r}
svm.overfit <- svm(y ~ ., data = data.train, kernel = "radial", gamma = 100, cost = 1, decision.values = T)
svm.plot(svm.overfit, data.train)
```
With a gamma parameter of $100$, there were `r length(svm.overfit$index)` support vectors.

## Cross-Validation using the function tune()

Want to perform a 10-fold cross-validation using **tune()** to select the best choice of $\gamma$ and the cost-parameter:
```{r}
# ?tune()
# ?tune.control()
set.seed(1)
tune.out <- tune(svm, y ~ ., data = data.train, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4))) 
## 25 models
## 10-fold cross-validation: Randomly split data.train into 10 test training data sets of size 10
## For each test data set: The remaining 90 observations are used to train the model. Its test error rate is determined on the test data. 
## For each model: We obtain 10 test error rates. Report the average and the standard deviation (referred to sa "dispersion")

## Values of the best parameters: tune.out$best.parameters
## Best model: tune.out$best.model
summary(tune.out)

```
The best parameters are cost = `r tune.out$best.parameters["cost"]` and $\gamma$ = `r tune.out$best.parameters["gamma"]`.

## Predictions using the function predict()
For the best model, we want to view the training and test set predictions using the function **predict()**:
```{r}
# SVM with best parameter: tune.out$best.model
# True class labels: data.train[,"y"]
# Predicted class label for each test observation: predict() outcome

truth <- data.train[,"y"]
pred <- predict(tune.out$best.model, data.train)
table(truth, pred)
```
Result: `r sum(truth != pred)`% of the training data were misclassified.
```{r}
# The test data set
truth <- data.test[, "y"]
pred <- predict(tune.out$best.model, data.test)
table(truth, pred)

```
Result: `r sum(truth != pred)`% of the test data were misclassified.

## ROC Curves
We want to assess the predictive power using ROC curves. The fitted values $f(x)$ serve as our continuous predictor for the binary classification.

```{r}
library("ROCR")

# Our best model is tune.out$best.model or
svmfit.opt <- svm(y ~ ., data = data.train, kernel = "radial", gamma = 2, cost = 1, decision.values = T)

# Want to compare this SVM to a highly flexible SVM
svmfit.flex <- svm(y ~ ., data = data.train, kernel = "radial", gamma = 50, cost = 1, decision.values = T)

# Calculate fitted values for training and test data for both models opt and flex
# Note again: Applying the function predict() yields only the predicted class labels. On has to set decision.values = TRUE and the fitted values can then be obtained via the attribute() function:
pred.opt.train <- attributes(predict(svmfit.opt, data.train, decision.values = TRUE))$decision.values
pred.opt.test <- attributes(predict(svmfit.opt, data.test, decision.values = TRUE))$decision.values
pred.flex.train <- attributes(predict(svmfit.flex, data.train, decision.values = TRUE))$decision.values
pred.flex.test <- attributes(predict(svmfit.flex, data.test, decision.values = TRUE))$decision.values

# Save the true class labels
truth.opt.train <- data.train[, "y"]
truth.opt.test <- data.test[, "y"]
truth.flex.train <- data.train[,"y"]
truth.flex.test <- data.test[,"y"]

# Plot the ROC curves using the following function:
rocplot <- function(pred, truth, ...) {
  # For a suitable range of cutoff values t, we have to calculate the true and false positive rates tpr(t) and fpr(t). This is done automatically by the performance() function:
  # Note: The function performance() only takes objects of class "prediction"!
   
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  
  #We then plot tpr(t) against fpr(t).
  plot(perf, ...)
}


par(mfrow = c(1, 2))
rocplot(pred.opt.train, truth.opt.train, main = "Training Data")
rocplot(pred.flex.train, truth.flex.train, add = T, col = "red")
rocplot(pred.opt.test, truth.opt.test, main = "Test Data")
rocplot(pred.flex.test, truth.flex.test, add = T, col = "red")
```

## SVM with Multiple Classes
If the response is a factor containing more than two levels, then the svm() function will perform a multi-class classification using the one-versus-one approach:

Suppose, we are in a $M$-class setting with $M>2$. 

**One-Versus-One Classification** (Pairwise classification):

* perform classification for all $\left(^{M}_{2}\right)$ pairs 
* classifiy a test observation using each of the $\left(^{M}_{2}\right)$ classifiers, tally the number of times that the test observation is assigned to each of the $M$ classes and assign the test observation to the class which it was most frequently assigned to in the pairwise classifications.
```{r}
set.seed(1)
# We add 50 observations of our two real-valued features blong to a third class 3:
y <- c(y, rep(3, 50))
x <- rbind(x, matrix(rnorm(50 * 2), ncol = 2))

# We move our class 3 observations in order to distinguish them from class 1 and class 2 observations:
x[y == 3, 2] <- x[y == 3, 2] + 2

# Convert simulated data in a data frame 
dat <- data.frame(x = x, y = as.factor(y))

# Plot the data
par(mfrow = c(1, 1))
plot(x[,2],x[,1], col = y )

# Fit an SVM to the data using again RBF with cost = 10 and gamma =1:
svm.out <- svm(y ~ ., data = dat, kernel = "radial", cost = 10, gamma = 1)
# Plot the model
# Note: svm.plot is not written for multi-class SVMs!
plot(svm.out, dat)

# Compare this to an SVM using a polynomial kernel:
tune.out <- tune(svm, y ~ ., data = dat, kernel = "polynomial", cost = 10, coef0 = 1, gamma = 1,ranges = list(degree = 1:5))
svm.out <- tune.out$best.model
plot(svm.out, dat)
```

# Application to Gene Expression Data
The Khan data set consists of a number of tissue samples corresponding to four distinct types of small round blue cell tumors (denoted by 1,2,3,4). For each tissue sample (observation), $p = 2.308$ gene expression measurements are available. There are $n= 83$ tissue samples split into $63$ training and $20$ test observations. 

```{r}
library("ISLR")

typeof(Khan)
class(Khan)
names(Khan)
# Dimension of training observations:
dim(Khan$xtrain)
# Dimension of test observations:
dim(Khan$xtest)
# Number of training labels:
length(Khan$ytrain)
# Number of test labels:
length(Khan$ytest)
# Tumor frequencies in training data:
table(Khan$ytrain)
# Tumor frequencies in the test data:
table(Khan$ytest)
```
**Discussion**: Based on these information, what type of kernel would you choose in order to fit a 4-class SVM classifier on the training data? Linear? Polynomial? Radial?

```{r}
# Data frames for training and test data:
data.train <- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
data.test <- data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))

# Fit a linear SVM!
# Why linear?
# Only few observations in high-dimensional feature space (p >> n)
# Consequence: It is very likely that there are four distinct (p-1)-dimensional hyperplanes each containing the observations of exactly one class (This is not true e.g. if all observations are on a single line)
# In this case: It is easy to find another four distinct hyperplanes separating one class from another
# Additional flexibility provided by non-linear kernels unnecessary
# Linear Kernel yields an estimator with low bias and low variance!
svm.out <- svm(y ~ ., data = data.train, kernel = "linear", cost = 10)
summary(svm.out)


# Training errors 
pred.train <- predict(svm.out, data.train)
truth.train <- data.train[, "y"]
table(pred.train, truth.train)

# Test errors
pred.test <- predict(svm.out, data.test)
truth.test <- data.test[,"y"]
table(pred.test, truth.test)
```
There were `r sum(truth.train != pred.train)` misclassifications in the training data and `r sum(truth.test != pred.test)` misclassifications in the test data.

# Session Info {-}


```{r}
sessionInfo()
```

