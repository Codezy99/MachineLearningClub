---
title: "Support Vector Machines (SVMs) -- Part II"
subtitle: "ISLR- chapter 9"
author: "Lars Hadidi, Manuel Herbst (maherbst@uni-mainz.de)</br>"
date: "2020/06/01</br>
  IMBEI - University Medical Center Mainz"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts","css/FMstyles.css","css/animate.css"]
    
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%/%total%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
      

---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  message = FALSE,
  echo = TRUE,
  warnings = FALSE,
  fig.align = "center"
)
```
---

class: middle, center
# 1.) Motivation: 
---

# Motivation
```{r echo = FALSE, out.width="70%"}
knitr::include_graphics(path = "SVM_files/fig 1.png")
```

---
# Motivation
## Separation using a straight line:
```{r echo = FALSE, out.width = "60%"}
knitr::include_graphics(path = "SVM_files/fig 2.png")
```
---
# Motivation
## Separation using a circle is better:
```{r echo = FALSE, out.width = "60%"}
knitr::include_graphics(path = "SVM_files/fig 3.png")
```

---
# Motivation
```{r echo = FALSE, out.width = "90%"}
knitr::include_graphics(path = "SVM_files/fig 4.png")
```

---
# Motivation
```{r echo = FALSE, out.width = "90%"}
knitr::include_graphics(path = "SVM_files/fig 5.png")
```
---

# Motivation
```{r echo = FALSE, out.width = "90%"}
knitr::include_graphics(path = "SVM_files/fig 6.png")
```
---
# Motivation
```{r echo = FALSE, out.width = "90%"}
knitr::include_graphics(path = "SVM_files/fig 6a.png")
```
---
class: middle, center

# 2.) Enlarging the feature space 

---
# Enlarging the feature space
 
```{r echo = FALSE, out.width = "90%"}
knitr::include_graphics(path = "SVM_files/fig 7.png")
```
The SVC algorithm would perform poorly: A single separating point is not enough. We are better served using two points.
---
# Enlarging the feature space
## Work in 2d!
```{r echo = FALSE, out.width = "75%"}
knitr::include_graphics(path = "SVM_files/fig 8.png")
```
---
# Enlarging the feature space
## solved by SVC in 2d
```{r echo = FALSE, out.width = "60%"}
knitr::include_graphics(path = "SVM_files/fig 9.png")
```

---
# Enlarging the feature space
Why $y = x^2$ and not $y = x^3$?

```{r echo = FALSE, out.width = "80%", out.height="80%"}
knitr::include_graphics(path = "SVM_files/fig 10.png")
```
---
# Enlarging the feature space
## General case: 
```{r echo = FALSE, out.width= "60%"}
knitr::include_graphics(path = "SVM_files/fig 11.png")
```
$\Phi$ maps the originial feature space ( $\mathbb{R}^p$ ) into some higher dimensional space (enlarged feature space).

Previous example: $\Phi: \mathbb{R} \to \mathbb{R}^2, \Phi(x) = (x,x^2)$

**Reduction to SVC-algorithm**:
Seperation via hyperplane in the enlarged feature space.

---

# Enlarging the feature space
## Problems

1. What is a good choice for $\Phi$?
2. huge computational effort (complex $\Phi$, high-dimensional problem)

### Recall the following notation:

+ $p$: number of (continuous) features
+ $n$: number of obserations (in our training data set)
+ $x_i = (x_{i1}, x_{i2}, \ldots, x_{ip})$: $i$-th observation in $\mathbb{R}^p$
+ $\left\langle a,b \right\rangle = a_1b_1 + a_2b_2 + \ldots + a_pb_p$ inner product in $\mathbb{R}^p$
+ $\left\|a\right\|^2 = a_1^2 + a_2^2 + \ldots + a_p^2$ squared length of a vector $a\in\mathbb{R}^p$
+ $\Phi$: feature map
---
# Enlarging the feature space
## Optimization problem 
Suppose, the enlarged feature space is given by $\mathbb{R}^q$ for some $q>p$. Replacing the $x_i$ by $\Phi(x_i)$, one obtains the SVC optimization problem in $\mathbb{R}^q$:
$$
\begin{equation*}
\underset{d,\beta\in\mathbb{R}^q,\epsilon\in\mathbb{R}^n,M}{\text{maximize}}\ \  M \\
\text{subject to}\ \ \left\|\beta\right\|^2 = 1, \\
y_i(d + \left\langle\beta,\Phi(x_i)\right\rangle )\geq M(1-\epsilon_i), \\
\epsilon_i \geq 0,\ \sum_{i=1}^n\epsilon_i \leq C
\end{equation*}
$$
If the optimal solution is attained: 

A **support vector** $x_i$ satisfies $y_i(d + \left\langle\beta,\Phi(x_i)\right\rangle ) = M(1-\epsilon_i)$. 

A **non-support vector** $x_j$ satisfies $y_j(d + \left\langle\beta,\Phi(x_j)\right\rangle ) > M$.
---

# Enlarging the feature space
## Reduce computational effort:
The optimal solution $\beta:= (\beta_1,\beta_2,\ldots,\beta_q)$ can be found in the linear subspace of $\mathbb{R}^q$ that is generated by the $n$ transformed observations $\Phi(x_1),\ldots,\Phi(x_n)$:
$$ \beta = \alpha_1\Phi(x_1) + \alpha_2\Phi(x_2) + \ldots + \alpha_n\Phi(x_n).$$
If $q>n$, then only $n$ coefficients need to be estimated! 

Moreover: **The $\alpha_i$'s are nonzero only for support vectors,** since only the support vectors contribute to the solution. 


---
# Enlarging the feature space
## Optimization problem for the $\alpha_i$
Substitution of $\beta= \alpha_1\Phi(x_1) + \alpha_2\Phi(x_2) + \ldots + \alpha_n\Phi(x_n)$ yields:
$$
\begin{equation*}
\underset{d,\alpha_i,\epsilon_i,M}{\text{maximize}}\ \  M \\
\text{subject to}\ \ \sum_{i,j=1}^n\alpha_i\alpha_j\left\langle\Phi(x_i),\Phi(x_j)\right\rangle = 1, \\
y_i\left(d + \sum_{j=1}^n\alpha_j\left\langle\Phi(x_j),\Phi(x_i)\right\rangle\right) \geq M(1-\epsilon_i), \\
\epsilon_i \geq 0,\ \sum_{i=1}^n\epsilon_i \leq C
\end{equation*}
$$
---
# Enlarging the feature space
## Computational advantage:
+ Estimate only $n+1$ parameters $d,\alpha_1,\alpha_2,\ldots,\alpha_n$ (instead of a huge number of $\beta$-coefficients)
+ Do not need explicit form of $\Phi$ (which may be very complex), only need to know the $\frac{1}{2}n(n+1)$ inner products $\left\langle\Phi(x_i),\Phi(x_j)\right\rangle$.
---
# Enlarging the feature space
## Questions:

+ **Machine learning`s view:** How does the algorithm "learn" what the "right" feature map $\Phi$ is and how many dimensions are to be added to the feature space?

+ **Programmer`s view:** How to compute the inner products $\left\langle\Phi(x_i),\Phi(x_j)\right\rangle$ without explicitly knowing the feature map $\Phi$?
---
class: middle, center

# 3.) Kernel maps and Support vector Machines (SVMs)

---
# Support Vector Machines (SVMs)
## Kernel maps
**A kernel map $K$ is a function that quantifies the similarity of two obervations.** 

**Conceptual interpretation:** Is a given observation $x$ "closer"/"more similar" to observation $x_i$ from class $y = +1$ or to observation $x_j$ from class $y = -1$ ? 

The SVM decides by comparing $K(x_i,x)$ with $K(x_j,x)$.
```{r echo = FALSE, out.width = "50%"}
knitr::include_graphics(path = "SVM_files/fig 12.png")
```

---
# Support Vector Machines (SVMs)
**Idea:** Replace inner product $\left\langle\Phi(x_i),\Phi(x_j)\right\rangle$ by a kernel map $K(x_i,x_j)$ and estimate the decision boundary as the zeros of
$$ f(x) = \alpha_0 + \alpha_1K(x_1,x) + \alpha_2K(x_2,x) + \ldots + \alpha_nK(x_n,x). $$
+ $n$ = number of observations 
+ $x_i$ = $i$-th obervation in the feature space $\mathbb{R}^p$
+ $K$ = kernel map with two arguments from feature space and $\mathbb{R}$-valued   output.
+ $\alpha_i$ = real coefficients to be estimated by SVM algorithm

**Note:** Linear in $\alpha$, but may be nonlinear in $x$ 

** The resulting classifier is known as a SVM.**


---
# Support Vector Machines (SVMs)
Instead of the feature map, the user has to specify a kernel map $K$. It is required that there exists a feature map $\Phi$ for which the kernel takes the form $K(x,y) =\left\langle\Phi(x),\Phi(y)\right\rangle.$

--

This requirement ensures that an SVM is equivalent to an SVC performed in some high-dimensional linear space (theoretical perspective). Properties of SVCs are inherited to SVMs (e.g. the Support Vector Property). 

--

However, the computational approach is somewhat different and more advantageous: One needs only compute $K(x_i,x_j)$ for all pairings. This can be done without explicitly working in the enlarged feature space. The feature map $\Phi$ is never explicitly calculated!  
Note: It may happen that the enlarged feature space is infinite-dimensional! 

--

A feature map $\Phi$ does not exist for the Euclidean distance $K(x,y) = \left\|x-y\right\|$.  

--
 
Necessary and sufficient criteria for the existence of $\Phi$ are provided by theorems like the **Representer Theorem** or **Mercers Theorem**. (Topic on its own!)  



---
class: middle, center

# 4.) Three popular choices of kernel maps (Existence of feature maps guaranteed!) 
---

# Three popular choices for $K$:
### The linear kernel $K(x,y) := \left\langle x,y \right\rangle$
* linear decision boundary
* gives us back the SVC

### The polynomial kernel $K(x,y) := (1+\left\langle x,y \right\rangle)^d$
* polynomial decision boundary of degree $d$
* The case $d=1$ gives us back the SVC

### The radial kernel $K(x,y) := \exp(-\gamma \left\|x-y\right\|^2)$
* highly flexible decision boundary with "local behavior"
* infinite-dimensional enlarged feature space
* additional tuning parameter $\gamma > 0$.
---

# Three popular choices for $K$
```{r echo = FALSE, out.width = "30%", fig.cap="ISLR-Example"}
knitr::include_graphics(path = "SVM_files/fig 14.png")
```
---

# Three popular choices for $K$
## Linear kernel
```{r echo = FALSE, out.width = "30%", fig.cap="Linear kernel yields the same decision boundary as SVC "}
knitr::include_graphics(path = "SVM_files/fig 15.png")
```
---

# Three popular choices for $K$
## Polynomial kernel
```{r echo = FALSE, out.width = "30%", fig.cap="Polynomial kernel of degree 3 "}
knitr::include_graphics(path = "SVM_files/fig 16.png")
```
---

# Three popular choices for $K$
## Radial kernel
```{r echo = FALSE, out.width = "30%", fig.cap="Radial kernel with unkown tuning parameter "}
knitr::include_graphics(path = "SVM_files/fig 17.png")
```
---
# Three popular choices for $K$
## How does the radial kernel work?
Given a test observation $x\in\mathbb{R}^p$. 
    
Predicted class label $y$ is based on the sign of $$f(x) = \alpha_0 + \sum_{i=1}^n\alpha_i\exp(-\gamma\left\|x_i-x \right\|^2).$$  
If $x$ is sufficiently far away from certain training data point $x_i$, then $\exp(-\gamma\left\|x_i-x\right\|^2) \approx 0$. Hence, only nearby training observations have an effect on the class label of a test observation ("local" behavior).

---
# Three popular choices for $K$
## The role of the tuning parameter $\gamma$
```{r echo = FALSE, out.width = "30%", fig.cap="Radial kernel with tuning parameter 50 "}
knitr::include_graphics(path = "SVM_files/fig 18.png")
```

---

# Three popular choices for $K$
## The role of the tuning parameter $\gamma$
```{r echo = FALSE, out.width = "30%", fig.cap="Radial kernel with tuning parameter 12,5 "}
knitr::include_graphics(path = "SVM_files/fig 19.png")
```
---

# Three popular choices for $K$
## The role of the tuning parameter $\gamma$
```{r echo = FALSE, out.width = "30%", fig.cap="Radial kernel with tuning parameter 3,125  "}
knitr::include_graphics(path = "SVM_files/fig 20.png")
```
---

# Three popular choices for $K$
## The role of the tuning parameter $\gamma$
```{r echo = FALSE, out.width = "30%", fig.cap="Radial kernel with tuning parameter 0,78  "}
knitr::include_graphics(path = "SVM_files/fig 21.png")
```
---

# Three popular choices for $K$
## The role of the tuning parameter $\gamma$
```{r echo = FALSE, out.width = "30%", fig.cap="Radial kernel with tuning parameter 0,2"}
knitr::include_graphics(path = "SVM_files/fig 22.png")
```
---

# Three popular choices for $K$
## The role of the tuning parameter $\gamma$
```{r echo = FALSE, out.width = "30%", fig.cap="Radial kernel with tuning parameter 0,05  "}
knitr::include_graphics(path = "SVM_files/fig 23.png")
```
---
# Three popular choices for $K$
## The role of the tuning parameter $\gamma$

* Large values: Low bias, high variance, risk of overfitting
* Small values: Higher bias, lower variance
---

# Three popular choices for $K$
## Underlying feature map
For a single feature ( $p=1$ ), the computation of the feature maps for the radial and polynomial kernel yields:
### Polynomial kernel of degree $d$:
$$\Phi(x) = \left(1,\sqrt{d}\cdot x, \sqrt{\frac{d(d-1)}{2}}\cdot x^2, \ldots,\sqrt{\left(^{d}_{k}\right)}\cdot x^k,\ldots, x^d\right)$$
### Radial kernel with tuning parameter $\gamma >0$
$$
\Phi(x) = \exp(-\gamma x^2)\left(1,\sqrt{2\gamma}\cdot x, \sqrt{\frac{4\gamma^2}{2}}\cdot x^2,\ldots,\sqrt{\frac{2^k\gamma^k}{k!}}\cdot x^k,\ldots\right)
$$
---
class: middle, center

# 5.) ROC curves: Assessing the quality of SVM as a classification model
---

# ROC curves
Both SVC and SVM estimate a function $\hat{f}$ and classify obseravtions $x$ according to the sign of $\hat{f}(x)$.

--

Now, for any given cutoff $t\in\mathbb{R}$, we classify observations $x$ as $$y = \begin{cases} -1, & \hat{f}(x) < t\\1, & \hat{f}(x)\geq t.\end{cases}$$

The ROC curve is obtained by computing the false positive and true positive rates for a range of values of $t$.
$$ \text{ROC}(t) := (\text{FPR}(t), \text{TPR}(t)) = (1-\text{specificity},\text{sensitivity})(t)$$

An optimal classifier will hug the top left corner (with coordinates $(1,1)$ ) of the ROC plot.
---

# ROC curves
## Example: Heart data
The aim is to use 13 predictors ( features $x$ ), such as *Age* or *Chol*, to predict whether an individual has heart disease ( $y$ ).

--

After removing 6 missing observations, the data consist of 297 subjects, which we randomly spit into 207 training and 90 test observations

--

We estimate $\hat{f}$ using SVC and radial kernel SVMs with tuning parameter $\gamma = (.001,.01,.1)$ on the training data and then plot the ROC curves:
+ first on the training data (training error) 
+ and then on the test data (test error/prediction error)

---
# ROC curves
```{r echo = FALSE, out.width = "90%", fig.cap="The SVC is compared to an SVM using a radial basis kernel with 3 different tuning parameters. Left: Training data, Right: Test data "}
# par(mfrow = c(2,1))
knitr::include_graphics(path = "SVM_files/fig 26.png")
```

---
# ROC curves
Training error ROC:
* As $\gamma$ increases and the fit becomes more non-linear, the ROC curves improve.
* Using $\gamma = .1$ appears to give an almost perfect ROC curve.

```{r echo = FALSE, out.width = "40%"}
# par(mfrow = c(2,1))
knitr::include_graphics(path = "SVM_files/fig 24.png")
```
---
# ROC curves

Test error ROC:
* Using $\gamma = .1$ produces the worst estimates
* evidence: While a more flexible method (low bias, high variance) will often produce lower training error rates, this does not necessarily lead to improved performance on test data.
* The SVMs using $\gamma = .01$ and $\gamma = .001$ perform comparably to SVC, and all three outperform the SVM with $\gamma = .1$

```{r echo = FALSE, out.width = "40%"}
# par(mfrow = c(2,1))
knitr::include_graphics(path = "SVM_files/fig 25.png")
```
---

# ROC curves
## Questions:

1. The heart data set contained **categorical** features (e.g. *SEX*). How do SVMs treat categorical features? Via stratified classification?
2. What, if the variable *heart disease* had more than two classes?
3. Comparison to other classification models (logistic regression, LDA, classification trees,...)?
4. **Missing data problem**: How do we proceed if observations are incomplete? (Missing class labels, missing features) 
---
class: middle, center

# 6.) SVMs with more than two classes

---
# SVMs with more than two classes
Suppose, we are in a $M$-class setting with $M>2$. The two most popular approaches are: 

## One-Versus-One Classification
* perform classification for all $\left(^{M}_{2}\right)$ pairs 
* classifiy a test observation using each of the $\left(^{M}_{2}\right)$ classifiers, tally the number of times that the test observation is assigned to each of the $M$ classes and assign the test observation to the class which it was most frequently assigned to in the pairwise classifications.


## One-Versus-All Classification
* compare one class $m$ (coded as $+1$!) to the remaining $M-1$ classes, yielding $M$ classifiers $\hat{f}_m$
* classify a test observation $x$ to the class $m$ for which $\hat{f}_m(x)$ is largest.

---
class: middle, center

# 7.) Relationship to Logistic Regression

---
# Relationship to Logistic Regression
## "Loss + Penalty" form

Deep connections between different supervised learning tools are revealed when transformed in the "Loss + Penalty" form $$\underset{f}{\text{minimize}}\ \ \sum_{i=1}^n L(y_i,x_i,f) + \lambda P(f).$$
* $L$ is some loss function quantifying the extent to which the model $f$ fits the data $(y_i,x_i)$
* $P$ is a penalty function which is large when $L$ is small and vice versa.
* The tuning parameter $\lambda$ regulates the bias-variance trade-off

---
# Relationship to Logistic Regression
## "Loss + Penalty" form for an SVC
The quadratic SVC optimization problem is equivalent to
```{r echo = FALSE, out.width = "80%"}
knitr::include_graphics(path = "SVM_files/fig 28.png")
```

<!-- The quadratic SVC optimization problem is equivalent to $$\underset{d,M,\beta}{\text{minimize}}\ \ \sum_{i=1}^n\underbrace{ \max\left(0,1-\frac{y_i}{M}\left(d+\left\langle\frac{\beta}{\left\|\beta\right\|},x_i\right\rangle\right)\right)}_{=\epsilon_i} + \lambda \frac{1}{M^2}$$ -->
Taking $f(x) = \frac{1}{M}\left(d+\left\langle\frac{\beta}{\left\|\beta\right\|},x\right\rangle\right)$, we obtain
* Loss function $L(y_i,x_i,f) = \max(0,1-y_if(x_i))$ (hinge loss) quantifies violations of the margin (slack variables)
* Penalty function $P(f) = \frac{1}{M^2}$ penalizes small margin widths
* small $\lambda$ allows small margin widths, only few violations to the margins will occur: low-bias-high-variance estimator
* large $\lambda$ penalizes small margin width, many violations to the margin are tolerated: low-variance-high-bias estimator

---
# Relationship to Logistic Regression
## SVC and logistic regression
"Loss+Penalty" form exists also for logistic regression and we compare the two loss functions
* Logistic Loss: $L(y,x,f) = \ln(1+ \exp(-yf(x)))$
* SVC Loss: $L(y,x,f) = \max(0,1-yf(x))$

---
# Relationship to Logistic Regression
```{r echo = FALSE, out.width = "30%", fig.cap="The SVC and logistic regression loss functions are compared, as a function of yf(x) "}
knitr::include_graphics(path = "SVM_files/fig 27.png")
```

* Due to the similarities between their loss functions, logistic regression and SVC often give very similar results.
* SVC loss is exactly zero for $yf(x)\geq 1$ (observations on the correct side of the margin). The Logistic loss is nowhere zero!
* When the classes are well separated, SVC/SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.
---
class:middle, center

# 8.) Summary
---
# Summary

Find the "curved" decision boundary whose margins can be extended as wide as possible such that the resulting violations of the margins do not exceed a specified threshold! 

```{r echo=FALSE, out.width="30%"}
knitr::include_graphics(path = "SVM_files/fig 6.5.png")
```
The "nonlinear" decision boundary curve is given by the zeros of a "nonlinear" function of the form $$f(x) = \alpha_0 + \alpha_1K(x_1,x) + \alpha_2K(x_2,x) + \ldots + \alpha_nK(x_n,x)$$ 

A **support vector** is an observation that violates the margin (including observations on the margin!). 


---
# Summary
```{r echo = FALSE, out.width = "60%", fig.cap="SVM algorithm produces the same decision boundary, using only support vectors"}
knitr::include_graphics(path = "SVM_files/fig 13.png")
```
**Support vector property**: If a support vector was moved slightly, then the decision boundary would move as well. A slight movement of an observation on the correct side of the margin does not affect the solution as long as it remains on the correct side of the margin. 
This property distinguishes SVCs/SVMs from other classfication tools.

---
# Summary 
## Important features of SVMs

1. Only support vectors contribute to the solution 
2. Use of kernel maps 
3. SVMs can be represented as SVCs in higher dimensional (enlarged) feature spaces
4. Effectively reduce amount of required computation
5. No assumptions on the distribution of the predictor variables
6. Do not estimate the role/importance/impact of single predictor variables (features)



---
class: middle, center

# Thanks!
