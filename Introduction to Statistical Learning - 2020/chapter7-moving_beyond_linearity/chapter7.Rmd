---
title: "An Introduction to Statistical Learning - with applications in R"
subtitle: "Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani"
author: "Irene Schmidtmann (Irene.Schmidtmann@uni-mainz.de)</br>"
date: "2020/06/08</br>
  IMBEI - University Medical Center Mainz"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts","css/FMstyles.css","css/animate.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  message = FALSE,
  echo = TRUE,
  warnings = TRUE,
  fig.align = "center"
)
```


```{r setup2, include=FALSE}
library(ISLR)
library(readr)
library(lattice)
library(knitr)
```

class: inverse, center, middle

# Chapter 7:  Moving Beyond Linearity

---
# Structure of chapter 7
Chapter 7 deals with several methods for tackling non-linear relationship between response and independent variable(s)

Today: 

7.1 Polynomial Regression

7.2 Step Functions (piecewise constant functions)

7.3 Basis Functions -- generalization of polynomial and piecewise-constant models

7.4 Regression Splines

Next week: 

7.5 Smoothing Splines // 7.6 Local Regression // 7.7 Generalized Additive Models

Note: The figures in this presentation are taken from "An Introduction to Statistical Learning, with applications in R"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani
---
#7.1  Polynomial Regression
Extend linear regression by replacing the standard linear model 

$$y_{i}=\beta_{0}+\beta_{1} x_{i}+\epsilon_{i}$$
with a polynomial function
$$y_{i}=\beta_{0}+\beta_{1} x_{i}+\beta_{2} x_{i}^{2}+\beta_{3} x_{i}^{3}+\ldots+\beta_{d} x_{i}^{d}+\epsilon_{i}$$
* Fitting model by least squares
* Large $d$ allows for very non-linear curves, but typically $d \le 4$
* For larger $d$ strange shapes may occur, especially at the boundaries of the range of the $X$ variable
---

#  Polynomial Regression: Examples

Fit polynomials of degree 4 to wage data and display fitted curves with 95% confidence intervals for estimates.
```{r Fig7_1, echo=FALSE, eval=TRUE, fig.height = 6, fig.width = 12, fig.cap = "Fig 7.1 Fit polynomial to wage data. Left: wage against age. Right: Modelling binary event 'high income' (wage > 250) using logistic regression.", fig.align = "left"}

par(mfcol=c(1,2))
# order by age (for easier plotting)
Wage2 <- Wage[order(Wage$age), ]
# add indicator for wage over 250K
Wage2 <- cbind(Wage2, Over250 <- ifelse(Wage2$wage > 250, 1, 0))
# fit regression model 
lm.age <- lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage2)
# compute confidence interval for prediction
predicted.age <- predict.lm(lm.age, interval = "confidence")
# Plot observed values, fitted values and confidence interval for fitted values
plot(x = Wage2$age, y=Wage2$wage, xlab = "Age", ylab = "Wage")
lines(x = Wage2$age, y=lm.age$fitted.values, col="blue", lwd = 2)
lines(x = Wage2$age, y=predicted.age[, "lwr"], col="blue", lty = 2, lwd = 2)
lines(x = Wage2$age, y=predicted.age[, "upr"], col="blue", lty = 2, lwd = 2)

# Fit logistic regression model for probability of earning 250K+
glm.fit <- glm(Over250 ~ cbind(age, age^2, age^3, age^4), family = binomial(link = "logit"), data = Wage2)
# Obtain predicted probability and standard errors
# on logit scale
predicted <- predict(glm.fit, type = "link", se.fit = TRUE)
# define inverse logit function
inverse_logit = function(x){
  exp(x)/(1+exp(x))
}
# transform to prob scale
se_high <- inverse_logit(predicted$fit + (predicted$se.fit*1.96))
se_low <- inverse_logit(predicted$fit - (predicted$se.fit*1.96))
expected <- inverse_logit(predicted$fit)

# plot predicted probability and 95% CI
plot(x = jitter(Wage2$age, 2), y = 0.2 * Wage2$Over250, pch="|", col = "grey", xlab = "Age", ylab ="P(Wage > 250 | Age)")
lines(x = Wage2$age, y = expected, type= "l", ylim = c(0, 0.2), lty=1, lwd=2, col = "blue")
lines(x = Wage2$age, y = se_low, type= "l", lty=2, lwd=2, col = "blue")
lines(x = Wage2$age, y = se_high , type= "l", lty=2, lwd=2, col = "blue")


```
---
#  Polynomial Regression: How to obtain confidence intervals for predictions?
The fitted value for a particular value $x_0$ is given by
$$\hat{f}\left(x_{0}\right)=\hat{\beta}_{0}+\hat{\beta}_{1} x_{0}+\hat{\beta}_{2} x_{0}^{2}+\hat{\beta}_{3} x_{0}^{3}+\hat{\beta}_{4} x_{0}^{4}$$
Let $\hat{C}$ the covariance matrix of the $\hat{\beta}_{j}$ and $\ell_{0}^{T}=\left(1, x_{0}, x_{0}^{2}, x_{0}^{3}, x_{0}^{4}\right)$, then 

$$\operatorname{Var}\left[\hat{f}\left(x_{0}\right)\right]=\ell_{0}^{T} \hat{\mathbf{C}} \ell_{0}$$
Obtain standard error of estimate by taking the square root and compute approximate 95% confidence interval as


$$\hat{f}\left(x_{0}\right) \pm 1.96 \sqrt{\operatorname{Var}\left[\hat{f}\left(x_{0}\right)\right]}$$
Similarly obtain confidence interval for linear predictor in logistic regression. Use inverse logistic transform to obtain predicted probability with confidence interval.
---
# 7.2 Step functions
* Polynomial functions as predictors impose *global structure* on the non-linear function of $X$.
* *Step functions* avoid this by effectively converting a continous variable into an *ordered categorical variable*.
* Specifiy cutpoints $c_{1}, c_{2}, \dots, c_{K}$ in the range of $X$ and define $K+1$ new variables:
$$\begin{aligned}
C_{0}(X) &=I\left(X<c_{1}\right) \\
C_{1}(X) &=I\left(c_{1} \leq X<c_{2}\right) \\
C_{2}(X) &=I\left(c_{2} \leq X<c_{3}\right) \\
& \vdots \\
C_{K-1}(X) &=I\left(c_{K-1} \leq X<c_{K}\right) \\
C_{K}(X) &=I\left(c_{K} \leq X\right)
\end{aligned}$$
where $I(\cdot)$ is an *indicator function*.
---

# 7.2 Step functions (continued)
* Use least squares to fit
$$y_{i}=\beta_{0}+\beta_{1} C_{1}\left(x_{i}\right)+\beta_{2} C_{2}\left(x_{i}\right)+\ldots+\beta_{K} C_{K}\left(x_{i}\right)+\epsilon_{i}$$
* Interpretation: 
  + $\beta_0$ is the mean of $Y$ for $X < c_1$.
  + $\beta_0 + \beta_j$ is the mean of $Y$ for $c_{j} \leq X < c_{j+1}$. Hence $\beta_j$ describes the average difference in the response between $X < c_1$ and $X \epsilon \left[ c_{j}, c_{j+1} \right)$
* Problem: how to choose the $c_j$ if there are no natural breakpoints?
* Nevertheless: popular in epidemiology, e.g.
  + five year age groups
  + quartiles or quintiles of covariates
---

# Step functions: examples
Fit step functions to wage data and display fitted functions with 95% confidence intervals for estimates.

```{r Fig7_2, echo=FALSE, eval=TRUE, out.width = "70%", fig.pos="h", fig.cap = "Fig 7.2 Fit step functions to wage data. Left: Weight against age. Right: Modelling binary event wage > 250 using logistic regression.", fig.align = "left"}
include_graphics(path = "images/Fig 7.2.png")
```
* Possible disadvantage: trends may be missed
---

# 7.3 Basis functions
Both polynomials and step functions are special cases of a more general approach, namely using *basis functions*.

Idea: Use a set of fixed functions applied to $X$, thus transforming $X$ suitably

$$y_{i}=\beta_{0}+\beta_{1} b_{1}\left(x_{i}\right)+\beta_{2} b_{2}\left(x_{i}\right)+\beta_{3} b_{3}\left(x_{i}\right)+\ldots+\beta_{K} b_{K}\left(x_{i}\right)+\epsilon_{i}$$
* Polynomial regression: $b_j(x_{i}) = x_{i}^{j}$
* Step functions: $b_{j}\left(x_{i}\right)=I\left(c_{j} \leq x_{i}<c_{j+1}\right)$
* Other types of functions are possible, e. g. splines.
* Still can determine coefficients $\beta_j$ by least squares.
---

# 7.4 Regression splines
## 7.4.1 Piecewise polynomials
* Combine ideas of polynomials and function defined piecewise: *pieceswise polynomial regression*,
  - typically low degree of polynomial, e. g. $d = 3$
  - e. g. for two parts of the range of $X$: fit two different polynomials
$$y_{i}=\left\{\begin{array}{ll}
\beta_{01}+\beta_{11} x_{i}+\beta_{21} x_{i}^{2}+\beta_{31} x_{i}^{3}+\epsilon_{i} & \text { if } x_{i}<c \\
\beta_{02}+\beta_{12} x_{i}+\beta_{22} x_{i}^{2}+\beta_{32} x_{i}^{3}+\epsilon_{i} & \text { if } x_{i} \geq c
\end{array}\right.$$

  - $K$ *knots* lead to $K+1$ separate polynomials
  - piecewise linear functions $(d=1)$ are also possible.

---

# Piecewise polynomials: example
```{r Fig7_3a, echo=FALSE, eval=TRUE, out.width = "50%", fig.pos="h", fig.cap = "Fig 7.3a Fit pieceswise polynomial to subset of wage data. Knot at age=50", fig.align = "left"}
include_graphics(path = "images/Fig 7.3a.png")
```

* 8 df are used as 8 parameters must be estimated
* Problem: No apparent reason for jump at age=50
* Discontinuity at knot should be remedied
---

## 7.4.2 Constraints and splines
* Discontinuity at knot can be avoided by imposing *constraints*:
fitted curve must be continuous, 
i. e. at the knot $x_k$ we must have
$$\beta_{0k}+\beta_{1k} x_{k}+\beta_{2k} x_{k}^{2}+\beta_{3k} x_{k}^{3} = \\
\beta_{0(k+1)}+\beta_{1(k+1)} x_{k}+\beta_{2(k+1)} x_{k}^{2}+\beta_{3(k+1)} x_{k}^{3}$$
---

## Piecewise polynomials with continuity constraint: example

```{r Fig7_3b, echo=FALSE, eval=TRUE, out.width = "50%", fig.pos="h", fig.cap = "Fig 7.3b Fit pieceswise polynomial to subset of wage data. Knot at age=50, continuity constraint", fig.align = "left"}
include_graphics(path = "images/Fig 7.3b.png")
```
* Better, but spike at knot still not satisfying, therefore: impose more constraints
---

## Smooth fit
Imposing further constraints for smoothness leads to (cubic) *splines*:
* Demand continous first derivative
$$\beta_{1k} + 2 \beta_{2k} x_{k} + 3 \beta_{3k} x_{k}^{2} = \beta_{1(k+1)} + 2 \beta_{2(k+1)} x_{k} + 3 \beta_{3(k+1)} x_{k}^{2}$$
* Demand continous second derivative
$$2 \beta_{2k} + 6 \beta_{3k} x_{k} = 2 \beta_{2(k+1)} + 6 \beta_{3(k+1)} x_{k}$$
* Each constraint frees up one df.
* So cubic spline with one knot only uses 5 df.
* More knots can be added.
* In general, a cubic splines with $K$ knots uses $4 + K$ df.
---

## Comparison of piecewise polynomials
```{r Fig7_3, echo=FALSE, eval=TRUE, out.width = "60%", fig.pos="h", fig.cap = "Fig 7.3 Various piecewise polynomials fitted to subset of wage data. Knot at age=50", fig.align = "left"}
include_graphics(path = "images/Fig 7.3.png")
```
---

## 7.4.3 The spline basis representation 
How to fit piecewise degree-d polynomial under continuity constraint on function and $d-1$ derivatives?

* Basis model can be used for cubic spline with $K$ knots can be modeled as
$$y_{i}=\beta_{0}+\beta_{1} b_{1}\left(x_{i}\right)+\beta_{2} b_{2}\left(x_{i}\right)+\cdots+\beta_{K+3} b_{K+3}\left(x_{i}\right)+\epsilon_{i}$$
for appropriate choice of basis functions $b_{1}, b_{2}, \dots, b_{K+3}$.
* Fit by least squares
* Most direct of many possible representations
  + basis for cubic polynomial $x, x^{2}, x^{3}$
  + add one *truncated power basis* function for each knot $\xi_k$, i. e.
$$h(x, \xi_k)=(x-\xi_k)_{+}^{3}=\left\{\begin{array}{cl}
(x-\xi_k)^{3} & \text { if } x>\xi_k \\
0 & \text { otherwise }
\end{array}\right.$$
  + adding *truncated power basis* functions will preserve continuity and continuous first and second order derivatives at knots
  + Thus fitting a cubic spline with $K$ knots amounts to least square regression with intercept and $3 + K$ predictors $$X, X^{2}, X^{3}, h\left(X, \xi_{1}\right), h\left(X, \xi_{2}\right), \ldots, h\left(X, \xi_{K}\right)$$
---

## Cubic spline: example
```{r Fig7_4, echo=FALSE, eval=TRUE, out.width = "50%", fig.pos="h", fig.cap = "Fig 7.4 Cubic spline fitted to subset of wage data. Three knots at ages 25, 40 and 60", fig.align = "left"}
include_graphics(path = "images/Fig 7.4.png")
```
* Problem with splines: high variance at extreme values of $X$
* Remedy: impose yet another constraint
  + Function must be linear at the boundary, i. e. if $X < \xi_1$ and $X > \xi_K$
  + Resulting function is called *natural spline* and has narrower confidence intervals
---

## 7.4.4 Choosing the number and locations of the knots
* Choice of location
  + High flexibility in regions with many knots $\rightarrow$ place many knots in area with rapid variation is expected, few where stability is expected
  + However, in practice knots are usually placed uniformly
      - specify df's (e. g. 4) and let software place knots at uniform quantiles (e. g. quartiles) of the predictor
* Choice of number
  + Trial and error: search for best looking curve
  + Cross validation: minimize RSS

---
## Cubic spline with 3 knots: example
```{r Fig7_5, echo=FALSE, eval=TRUE, out.width = "60%", fig.pos="h", fig.cap = "Fig 7.4 Cubic spline with knots at age quartiles fitted to wage data. Left: wage against age. Right: Modelling binary event 'high income' (wage > 250) using logistic regression.", fig.align = "left"}
include_graphics(path = "images/Fig 7.5.png")
```
---

## Cubic spline choice of number of knots: example
```{r Fig7_6, echo=FALSE, eval=TRUE, out.width = "70%", fig.pos="h", fig.cap = "Fig 7.6 Ten-fold cross-validated mean squared errors for selecting the degrees of freedom when fitting splines to the wage data. Left: natural spline. Right: cubic spline.", fig.align = "left"}
include_graphics(path = "images/Fig 7.6.png")
```
* 1 df (linear regression) inadequate
* Suitable df's
  + 3 df for natural spline
  + 4 df for cubic spline
---

## 7.4.5 Comparison to polynomial regression
Regression splines often give better results then polynomial regression

* Flexibility by introducing many knots instead of high degrees of polynomials
* Extra-flexibility of high degrees polynomials produce spurious results at boundaries
* Splines produce more stable estimates
* Knots can be chosen according to expected behaviour of function
---

class: middle, center

# Thanks!

