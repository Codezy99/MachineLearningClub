---
title: "LinReg_slidy"
author: "Philipp Mildenberger"
date: "16 März 2020"
output: slidy_presentation
---
<!-- --- -->
<!-- title: "Chapter 3 Linear Regression" -->
<!-- subtitle: "" -->
<!-- author: "Jan Linke </br> Philipp Mildenberger</br>" -->
<!-- date: "2020/03/16</br> -->
<!--   IMBEI - University Medical Center Mainz" -->
<!-- output:  -->
<!--   xaringan::moon_reader: -->
<!--     css: ["default", "default-fonts","css/animate.css"] -->
<!--     lib_dir: libs -->
<!--     nature: -->
<!--       highlightStyle: github -->
<!--       highlightLines: true -->
<!--       countIncrementalSlides: false -->
<!--       titleSlideClass: [center, middle] -->
<!-- --- -->

```{r setup, include=FALSE}
library(rgl)
library(knitr)
library(pander)
panderOptions("missing","")

options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  message = FALSE,
  echo = FALSE,
  warnings = FALSE,
  fig.align = "center",
  warning = FALSE,
  error = FALSE
)
opts_knit$set(root.dir = "../..") ## knit in grand-parent folder for the sake of easy (and consistent!) data access
```


$$\newcommand\xbar{\overline{x}}
\def\ybar{\overline{y}}
\def\eps{\epsilon}$$

# Chapter Overview

- Motivating Example
- Simple Linear Regression
- Multiple Linear Regression
- Other Considerations in the Regression Model
- Example expained
- Comparison to $K$-Nearest Neighbors


# Motivating Example
## Advertising Data
```{r, echo=FALSE}
Advertising <- read.csv("Introduction to Statistical Learning - 2020/chapter3-linear_regression/data/Advertising.csv")
```

```{r}
pander(head(Advertising))
```


# Motivating Example
## Questions

* Is there a relationship between advertising budget and sales?
* How strong is the relationship between advertising budget and sales?
* Which media contribute to sales?
* How accurately (and precisely) can we estimate the effect of each medium?
* How accurately can we predict future sales?
* Is the relationship linear?
* Is there synergy (i.e. interaction) among the advertising media?



# Simple Linear Regression
Predict quantitative response $Y$ on the basis of $X$

$$ Y \approx \beta_0 + \beta_1 X $$

- $\beta_0$ is called "Intercept"  and $\beta_1$ "slope". Both are often called "coefficients" "parameters"
- Both are unknown, will be estimated

With estimates $\hat\beta_0$  ,  $\hat\beta_1$, one can then make predictions 
for $Y$ based on observed values of $X$:

$$ \hat y = \hat \beta_0 + \hat\beta_1 x $$

# Estimating the Coefficients $\beta_0$ and $\beta_1$ 
- Let $y_i:=\hat \beta_0 + \hat\beta_1 x_i$ be the prediction for $Y$ based on the $i$-th value of $X$.
- The residual sum of squares (RSS) is defined as

$$\text{RSS}:= (y_1 - \hat y_1)^2 + \ldots + (y_n - \hat y_n)^2$$ 

- We want to minimise the RSS ("Least Squares").  
Letting $\frac{d\text{RSS}}{d\beta} \stackrel{!}= 0$ yields

\begin{aligned}
\hat \beta_1 =& \frac{\sum_{i=1}^n(x_i-\xbar)(y_i - \ybar)}
{\sum_{i=1}^n(x_i-\xbar)^2} \\
\hat \beta_0 =& \ybar - \hat \beta_1 \cdot \xbar
\end{aligned}
 

with $\ybar:=\frac{1}{n}\sum_{i=1}^n y_i$ and $\xbar:=\frac{1}{n}\sum_{i=1}^n x_i$ the sample means. These are direct (i.e. non-iterative) calculations :)

# Advertising Example
```{r}
plot(sales ~ TV, data=Advertising, pch=16, col=2)
```

* Scatterplot of `sales` and `TV`

# Advertising Example
```{r}
lmTV <- lm(sales~TV, data=Advertising)
coefs <- lmTV$coefficients
plot(sales ~ TV, data=Advertising, pch=16, col=2)
abline(a=coefs[1],b=coefs[2], lwd=2, col=4)
```

* Scatterplot of `sales` and `TV`
* Best linear fit to the data 

# Advertising Example
```{r}
pred  <- predict(lmTV)
plot(sales ~ TV, data=Advertising, pch=16, col=2)
abline(a=coefs[1],b=coefs[2], lwd=2, col=4)
with(Advertising,segments(TV, sales, TV, pred, lwd = 1))
```

* Scatterplot of `sales` and `TV`
* Best linear fit to the data 
* Residuals of the linear regression model

# Advertising Example
```{r}
pred  <- predict(lmTV)
plot(sales ~ TV, data=Advertising, pch=16, col=2)
abline(a=coefs[1],b=coefs[2], lwd=2, col=4)
with(Advertising,segments(TV, sales, TV, pred, lwd = 1))
```

```{r, echo=TRUE}
lm(sales~TV, data=Advertising)
```

# Assessing Accuracy of Coefficient Estimates

We want to know how much randomness went into a particular estimate produced by
the estimator $\hat\beta_1$. The standard error quantifies that:

$$ 
\text{SE}(\hat\beta_1)^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\xbar)^2} \qquad 
\text{SE}(\hat\beta_0)^2=\sigma^2\left[\frac 1 n + \frac{\xbar^2}{\sum_{i=1}^n(x_i-\xbar)^2}\right]
$$
where $\sigma^2:=\text{Var}(\eps)$ is the variance of the the random error term. 

* We observe
    * Higher $\sigma^2$ leads to larger SE
    * Higher Variance in $X$ leads to smaller SE  
* $\sigma^2$ is unknown, but can be estimated with the residuals by 

$$\hat\sigma^2:=\frac{RSS}{(n-2)}=\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{(n-2)}.$$

* The $-2$ in the denominator comes from the two degrees of freedom of the linear model.  
    + standard error increases with number of predictors
* By replacing $\sigma^2$ with $\hat \sigma^2$ in the formulae above, 
we get an estimator $\widehat{\text{SE}}(\hat\beta_1)$ for $\text{SE}(\hat\beta_1)$. 
This estimator can then be used for the construction of *confidence intervals* or for *hypothesis testing*.  


# Example to Accuracy Assessment
Assume the the true model has the form 
$$ 
Y = 2 + 3\cdot X + \eps \qquad \eps \stackrel{i.i.d.}{\sim}N(0,4^2)
$$ 

```{r}
set.seed(40)
par(mfrow=c(1,2))
N <- 50
x <- rnorm(N)
y <- 2+3*x + rnorm(N,0,4)
plot(x,y)
abline(2,3,col="red",lwd=2)
lm0 <- lm(y~x)
abline(lm0,col="blue",lwd=2)
RSS <- sum(lm0$residuals^2)
# sum(lm0$residuals^2)/(N-2)

x_cent_sq <- sum((x-mean(x))^2)
# sqrt((RSS/(N-2))/x_cent_sq)

plot(x,y,type="n")
B1 <- numeric(N)
sig_sq <- numeric(N)
for(k in 1:100){
  x <- rnorm(N)
  y <- 2+3*x + rnorm(N,0,4)
  lm1 <- lm(y~x)
  abline(lm1,col=rgb(0,0,1,0.2))
  B1[k]     <- lm1$coefficients["x"]
  sig_sq[k] <- sum(lm1$residuals^2)/(N-2)
}
abline(2,3,col="red",lwd=2)

```
Left: sample of 50 observations. True regression line in red, sample based
regression line in blue.  
Right: sample-based regression lines of 100 different samples

The estimated standard error of $\hat\beta_1$ is in this case:

$$
\begin{aligned}
\hat\sigma^2 =& \frac{RSS}{n-2} \approx \frac{679.2}{48}= 14.15 \\
\widehat{SE}(\hat\beta_1) =& \sqrt{\frac{RSS}{\sum_{i=1}^n(x_i-\xbar)^2}} 
\approx \sqrt{\frac{14.15}{55.7}} \approx 0.504
\end{aligned}
$$ 

... which is the same as r shows us:
```{r, echo=TRUE}
pander::pander(lm0)
```

We saw above, that the estimate for $\sigma=\sqrt{14.15}\approx 3.76$ 
was a bit lower than the actual value of $4$. 
If we look at the 100 different samples, we get 100 estimates that vary around
the true value. Their overall mean is `r mean(sqrt(sig_sq))`.  
That means: $\hat\sigma$ estimates $\sigma$ without bias.


<!-- ```{r} -->
<!-- mean(B1) -->
<!-- sd(B1-3) -->
<!-- qqnorm(B1) -->
<!-- ``` -->

# Hypothesis Testing 

Standard error can be used to conduct *hypothesis tests* on the coefficients. 
Often, one wants to assess wheter there is an relationship between $X$ and $Y$, 
this is the same as testing whether the slope is equal to zero. 

* **$H_0$**:  $\beta_1=0$ i.e. there is no relationship between $X$ and $Y$
* **$H_A$**:  $\beta_1\neq0$ i.e. there is some relationship between $X$ and $Y$

Under the assumption that the null hypothesis is true,
we get a *t-statistic* with $n-2$ degrees of freedom.
$$
t=\frac{\hat\beta_1 - 0}{\widehat{\text{SE}}(\hat\beta_1)} \qquad
$$
This is $t$-distributed since we needed to estimate the standard error. 

# Assessing the Accuracy of the Model

**Residual Standard Error (RSE)**

The RSE is an estimate of the standard deviation of the random error $\eps$.
It is the average amount the response will deviate from the true regression line.

$$
\text{RSE}=\sqrt{\frac{1}{n-2}\text{RSS}}
$$


**Variance Explained $R^2$**  

$R^2$ expresses the fraction of variance explained in $Y$. 

$$ 
R^2 = \frac{\text{TSS}-\text{RSS}}{\text{TSS}} = 1-\frac{\text{RSS}}{\text{TSS}}
$$
We set $TSS:=\sum_{i=1}^n(y_i-\ybar)^2$ the total sum of squares.  
In simple linear regression, $R^2$ equals the squared Pearson correlation.

**RSE versus $R^2$**
$R^2$ is a relative value between 0 and 1,  
RSE is an absolute value to be interpreted in the units of the respone.

# Multiple Linear Regression
When there is more than one predictor, we could do many simple linear regressions. In the Advertising example:


```{r}
pander::pander(lm(sales~TV, data=Advertising))
```

<hr>

```{r}
pander::pander(lm(sales~radio, data=Advertising))
```

<hr>

```{r}
pander::pander(lm(sales~newspaper, data=Advertising))
```

<hr> 

BUT:  

* We end up with multiple (possibly contradicting) predictions for one observation (market)
* It ignores the relationship between the predictors

# Multiple Linear Regression

So with multiple predictors the model is:

$$ 
Y=\beta_0 + \beta_1X_1 + \cdots + \beta_pX_p + \eps
$$ 

We interpret $\beta_j$ as the average eﬀect on $Y$ of a one unit increase in $X_j$,
holding all other predictors ﬁxed.

Similar to simple linear regression one can make predictions using the formula

$$
\hat y_i  = \hat \beta_0 + \hat\beta_1x_{i1} + \cdots + \hat\beta_px_{ip}
$$

where the estimators are obtained by minimizing the RSS:

$$
\text{RSS}:= (y_1 - \hat y_1)^2 + \ldots + (y_n - \hat y_n)^2
$$

# Multiple Linear Regression Advertising Example 


```{r, fig.width=12}
par(mfrow=c(1,2))
pred  <- predict(lmTV)

plot(sales ~ TV, data=Advertising, pch=16, col=2)
abline(a=coefs[1], b=coefs[2], lwd=2, col=4)
with(Advertising,segments(TV, sales, TV, pred, lwd = 1))

lmRadio <- lm(sales~radio, data=Advertising)
pred  <- predict(lmRadio)
coefs <- lmRadio$coefficients
plot(sales ~ radio, data=Advertising, pch=16, col=2)
abline(a=coefs[1], b=coefs[2], lwd=2, col=4)
with(Advertising, segments(radio, sales, radio, pred, lwd = 1))
par(mfrow=c(1,1))
```


# Multiple Linear Regression Advertising Example 

```{r, fig.width=12}
lmTVRadio <- update(lmTV, .~ . + radio) ## add second predictor

coefs <- coef(lmTVRadio)
a <- coefs["TV"]
b <- coefs["radio"]
c <- -1
d <- coefs["(Intercept)"]

AA <- BB <- Advertising[,c("TV","radio","sales")]
BB$sales <- lmTVRadio$fitted.values
CC <- rbind.data.frame(AA,BB)
CC <- CC[order(CC$TV,CC$radio),]

mfrow3d(nr = 1, nc = 2)
plot3d(sales ~ TV + radio, data=Advertising, col=2, size=5, box=FALSE)
rglplanes   <- planes3d(a, b, c, d, alpha = 0.5, col = "lightblue")
rglsegments <- segments3d(CC,col=1, lwd=1.5, alpha=0.8)

## with interaction
lmTVRadioInteract <- update(lmTVRadio, .~ . + TV:radio)

gridTV    <- with(Advertising, seq(min(TV), max(TV), length.out = 50))
gridRadio <- with(Advertising, seq(min(radio), max(radio), length.out = 50))
gridSales <- predict.lm(lmTVRadioInteract,
                        expand.grid(TV=gridTV,radio=gridRadio))
gridSales <- matrix(gridSales, nrow=50)

AA <- BB <- Advertising[,c("TV","radio","sales")]
BB$sales <- lmTVRadioInteract$fitted.values
CC <- rbind.data.frame(AA,BB)
CC <- CC[order(CC$TV,CC$radio),]

plot3d(sales ~ TV + radio, data=Advertising, col=2, size=5, box=FALSE)
persp3d(gridTV, gridRadio, gridSales, add=TRUE, col="lightblue", alpha=.5)
rglsegments <- segments3d(CC,col=1, lwd=1.5, alpha=0.8)
rglwidget()

## Adding TV^2 as additional predictor is worth it !! 
lmTVRadioInteractTV2 <- update(lmTVRadioInteract, .~.+ I(TV^2))
summary(lmTVRadioInteractTV2)
anova(lmTVRadioInteract,lmTVRadioInteractTV2)
##
```

# Multiple Linear Regression Advertising Example 

The coefficients for a model for `sales` with `TV`, `radio` and `newspaper` 
look like this:
```{r}
lmTVRadioNews <- update(lmTVRadio, .~.+newspaper)
pander::pander(lmTVRadioNews)
```

`newspaper` seems no longer to have an effect on `sales`. The correlation between `newspaper` and `radio` is $0.35$.

```{r}
cors <- cor(Advertising[,c("TV","radio","newspaper","sales")])
cors[lower.tri(cors)] <- NA
pander::pander(cors)
```

# Some Important Questions

1. Is at least one of the predictors useful in predicting the response?
2. Do all the predictors help explain $Y$, or is only a subset of the predictors useful?
3. How well does the model fit the data?
4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?

# 1. Is there a Relationship Between the Repsone and Predictors?


\begin{aligned} 
H_0 :& \qquad \beta_1=\beta_2=\cdots=\beta_p=0\\
H_A :& \qquad \text{At least one $\beta_j$ is non-zero}
\end{aligned}


Use the $F$-statistic:

$$
F=\frac{(\text(TSS)-\text{RSS})/p}{\text{RSS}/(n-p-1)} \quad \sim F_{p,n-p-1}
$$

* avoids multiple tests for each predictor
* If subset of predictors is taken for granted, the $F$-statistic can also be 
uesd to test whether additional parameters are useful.

# 2. Do we need all predictors?

* First we need am method to judge the quality of models
    * $F$-test, AIC, BIC, adjusted $R^2$, ... 
    * all methods have in common that they introduce some 'penalty' for the number of predictors
    
* Then, we need a procedure to models of different subsets of predictors
    * compare all subsets
    * forward selection
    * backward selection
    * mixed forward&backward selection

# 3. How well does the model fit the data?

Most common are RSE and $R^2$, which we've seen for simple linear regression

## $R^2$ 
```{r}
r_sq <- function(lm) round(summary(lm)$r.squared,3)
```

|   |   |   |
|--:|--:|--:|
| TV:  $`r r_sq(lmTV)`$ | Radio:  $`r r_sq(lmRadio)`$ | Newspaper:  $`r r_sq(lm(sales~newspaper, data=Advertising))`$ |
| TV&Radio:  $`r r_sq(lmTVRadio)`$ | Radio&Newspaper:  $`r r_sq(lm(sales~radio+newspaper, data=Advertising))`$| TV&Newspaper:  $`r r_sq(lm(sales~TV+newspaper, data=Advertising))`$|
| | TV&Radio&Newspaper:  $`r r_sq(lmTVRadioNews)`$ ||



## RSE 

```{r}
rse <- function(lm) round(sqrt(sum(lm$residuals^2) / lm$df), 3)
```

|   |   |   |
|--:|--:|--:|
| TV:  $`r rse(lmTV)`$ | Radio:  $`r rse(lmRadio)`$ | Newspaper:  $`r rse(lm(sales~newspaper, data=Advertising))`$ |
| TV&Radio:  $`r rse(lmTVRadio)`$ | Radio&Newspaper:  $`r rse(lm(sales~radio+newspaper, data=Advertising))`$| TV&Newspaper:  $`r rse(lm(sales~TV+newspaper, data=Advertising))`$|
| | TV&Radio&Newspaper:  $`r rse(lmTVRadioNews)`$ ||

* Newspaper leads little or no change in $R^2$ and to higher *RSE* even.
* Note: Adding an additional predictor cannot decrease $R^2$ (by design). 

# 4. How accurate are our predictions?

Given some values for $X$ we can make predictions for $Y$. 
If, for example, 100000 is spent on `TV` and 20000 is spent on `radio`, the expected `sales` are $11.26$ units:

$$
\hat y = \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 x_2 = 
\hat\beta_0 + \hat\beta_1 \mathtt{TV} + \hat\beta_2 \mathtt{radio} = 11.26
$$

There are three levels of uncertainty:
1. Reducible error: $\hat beta$s are estimates
2. Model bias: The true relationship is seldom exactly linear
3. Irreducible error: The random error term $\eps$ 


```{r}
predict(lmTVRadio, 
        data.frame(TV=100,radio=20,newspaper=0), 
        interval = "prediction")
predict(lmTVRadio, 
        data.frame(TV=100,radio=20,newspaper=0), 
        interval = "confidence")


```


# Small Excursus: Matrix Notation and Hat Matrix

Linear model in matrix notation: 

$$
\underbrace{\left(\begin{smallmatrix} y_1 \\ \vdots \\ y_n \end{smallmatrix}\right)}_{=:Y} =
\underbrace{\left(\begin{smallmatrix} 
1 & x_{11} & \ldots & x_{1p} \\ 
\vdots & \vdots & & \vdots \\ 
1 & x_{n1} & \ldots & x_{np}
\end{smallmatrix}\right)}_{=:X} \cdot 
\underbrace{\left(\begin{smallmatrix}
\beta_0 \\ \vdots \\ \beta_p 
\end{smallmatrix}\right)}_{=:\beta} +
\underbrace{\left(\begin{smallmatrix}
\eps_1 \\ \vdots \\ \eps_n
\end{smallmatrix}\right)}_{\stackrel{i.i.d.}\sim \,\,\, N(0,\sigma^2)}
$$

$X$ is called *design matrix*. The least squares estimator for the coefficients can then be expressed as 

$$ 
\hat \beta = \underbrace{(X^T X)^{-1}X^T }_{\small{\text{ Pseudoinverse of }} X} \cdot Y
$$ 

The predictions for $Y$ based on $X$ are then

$$
\hat Y = X\hat \beta = X (X^T X)^{-1}X^T \cdot Y := H \cdot Y
$$

where $H$ is called *prediction* or *hat matrix*. $H$ contains a lot of useful information, most importantly: The values on the main diagonal are the **leverages** of each observation.


# Other Considerations



# Example Explained




# Done!

