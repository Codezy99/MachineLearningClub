---
title: "LinReg_slidy"
author: "Philipp Mildenberger"
date: "16 März 2020"
output: slidy_presentation
---
<!-- --- -->
<!-- title: "Chapter 3 Linear Regression" -->
<!-- subtitle: "" -->
<!-- author: "Jan Linke </br> Philipp Mildenberger</br>" -->
<!-- date: "2020/03/16</br> -->
<!--   IMBEI - University Medical Center Mainz" -->
<!-- output:  -->
<!--   xaringan::moon_reader: -->
<!--     css: ["default", "default-fonts","css/animate.css"] -->
<!--     lib_dir: libs -->
<!--     nature: -->
<!--       highlightStyle: github -->
<!--       highlightLines: true -->
<!--       countIncrementalSlides: false -->
<!--       titleSlideClass: [center, middle] -->
<!-- --- -->

```{r setup, include=FALSE}
# library(rgl)
library(knitr)
library(pander)

options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  message = FALSE,
  echo = FALSE,
  warnings = FALSE,
  fig.align = "center",
  warning = FALSE,
  error = FALSE
)
opts_knit$set(root.dir = "../..") ## knit in grand-parent folder for the sake of easy (and consistent!) data access

## fuer 3d graphik
# knitr::knit_hooks$set(webgl=hook_webgl)
```


$$\newcommand\xbar{\overline{x}}
\def\ybar{\overline{y}}
\def\eps{\epsilon}$$

# Chapter Overview

- Motivating Example
- Simple Linear Regression
- Multiple Linear Regression
- Other Considerations in the Regression Model
- Example expained
- Comparison to $K$-Nearest Neighbors


# Motivating Example
## Advertising Data
```{r, echo=FALSE}
Advertising <- read.csv("Introduction to Statistical Learning - 2020/chapter3-linear_regression/data/Advertising.csv")
```

```{r}
pander(head(Advertising))
```


# Motivating Example
## Questions

* Is there a relationship between advertising budget and sales?
* How strong is the relationship between advertising budget and sales?
* Which media contribute to sales?
* How accurately (and precisely) can we estimate the effect of each medium?
* How accurately can we predict future sales?
* Is the relationship linear?
* Is there synergy (i.e. interaction) among the advertising media?



# Simple Linear Regression
Predict quantitative response $Y$ on the basis of $X$

$$ Y \approx \beta_0 + \beta_1 X $$

- $\beta_0$ is called "Intercept"  and $\beta_1$ "slope". Both are often called "coefficients" "parameters"
- Both are unknown, will be estimated

With estimates $\hat\beta_0$  ,  $\hat\beta_1$, one can then make predictions 
for $Y$ based on observed values of $X$:

$$ \hat y = \hat \beta_0 + \hat\beta_1 x $$

# Estimating the Coefficients $\beta_0$ and $\beta_1$ 
- Let $y_i:=\hat \beta_0 + \hat\beta_1 x_i$ be the prediction for $Y$ based on the $i$-th value of $X$.
- The residual sum of squares (RSS) is defined as

$$\text{RSS}:= (y_1 - \hat y_1)^2 + \ldots + (y_n - \hat y_n)^2$$ 

- We want to minimise the RSS ("Least Squares").  
Letting $\frac{d\text{RSS}}{d\beta} \stackrel{!}= 0$ yields

\begin{aligned}
\hat \beta_1 =& \frac{\sum_{i=1}^n(x_i-\xbar)(y_i - \ybar)}
{\sum_{i=1}^n(x_i-\xbar)^2} \\
\hat \beta_0 =& \ybar - \hat \beta_1 \cdot \xbar
\end{aligned}
 

with $\ybar:=\frac{1}{n}\sum_{i=1}^n y_i$ and $\xbar:=\frac{1}{n}\sum_{i=1}^n x_i$ the sample means

# Advertising Example
```{r}
plot(sales ~ TV, data=Advertising, pch=16, col=2)
```

* Scatterplot of `sales` and `TV`

# Advertising Example
```{r}
lm1 <- lm(sales~TV, data=Advertising)
coefs <- lm1$coefficients
plot(sales ~ TV, data=Advertising, pch=16, col=2)
abline(a=coefs[1],b=coefs[2], lwd=2, col=4)
```

* Scatterplot of `sales` and `TV`
* Best linear fit to the data 

# Advertising Example
```{r}
pred  <- predict(lm1)
plot(sales ~ TV, data=Advertising, pch=16, col=2)
abline(a=coefs[1],b=coefs[2], lwd=2, col=4)
with(Advertising,segments(TV, sales, TV, pred, lwd = 1))
```

* Scatterplot of `sales` and `TV`
* Best linear fit to the data 
* Residuals of the linear regression model

# Advertising Example
```{r}
pred  <- predict(lm1)
plot(sales ~ TV, data=Advertising, pch=16, col=2)
abline(a=coefs[1],b=coefs[2], lwd=2, col=4)
with(Advertising,segments(TV, sales, TV, pred, lwd = 1))
```

```{r, echo=TRUE}
lm(sales~TV, data=Advertising)
```

# Assessing Accuracy of Coefficient Estimates

We want to know how much randomness went into a particular estimate produced by
the estimator $\hat\beta_1$. The standard error quantifies that:

$$ 
\text{SE}(\hat\beta_1)^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\xbar)^2} \qquad 
\text{SE}(\hat\beta_0)^2=\sigma^2\left[\frac 1 n + \frac{\xbar^2}{\sum_{i=1}^n(x_i-\xbar)^2}\right]
$$
where $\sigma^2:=\text{Var}(\eps)$ is the variance of the the random error term. 

* We observe
    * Higher $\sigma^2$ leads to larger SE
    * Higher Variance in $X$ leads to smaller SE  
* $\sigma^2$ is unknown, but can be estimated with the residuals by 

$$\hat\sigma^2:=\frac{RSS}{(n-2)}=\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{(n-2)}.$$

* The $-2$ in the denominator comes from the two degrees of freedom of the linear model.  
    + standard error increases with number of predictors
* By replacing $\sigma^2$ with $\hat \sigma^2$ in the formulae above, 
we get an estimator $\widehat{\text{SE}}(\hat\beta_1)$ for $\text{SE}(\hat\beta_1)$. 
This estimator can then be used for the construction of *confidence intervals* or for *hypothesis testing*.  


# Example to Accuracy Assessment
Assume the the true model has the form 
$$ 
Y = 2 + 3\cdot X + \eps \qquad \eps \stackrel{i.i.d.}{\sim}N(0,4^2)
$$ 

```{r}
set.seed(40)
par(mfrow=c(1,2))
N <- 50
x <- rnorm(N)
y <- 2+3*x + rnorm(N,0,4)
plot(x,y)
abline(2,3,col="red",lwd=2)
lm0 <- lm(y~x)
abline(lm0,col="blue",lwd=2)
RSS <- sum(lm0$residuals^2)
# sum(lm0$residuals^2)/(N-2)

x_cent_sq <- sum((x-mean(x))^2)
# sqrt((RSS/(N-2))/x_cent_sq)

plot(x,y,type="n")
B1 <- numeric(N)
sig_sq <- numeric(N)
for(k in 1:100){
  x <- rnorm(N)
  y <- 2+3*x + rnorm(N,0,4)
  lm1 <- lm(y~x)
  abline(lm1,col=rgb(0,0,1,0.2))
  B1[k]     <- lm1$coefficients["x"]
  sig_sq[k] <- sum(lm1$residuals^2)/(N-2)
}
abline(2,3,col="red",lwd=2)

```
Left: sample of 50 observations. True regression line in red, sample based
regression line in blue.  
Right: sample-based regression lines of 100 different samples

The estimated standard error of $\hat\beta_1$ is in this case:

$$
\begin{aligned}
\hat\sigma^2 =& \frac{RSS}{n-2} \approx \frac{679.2}{48}= 14.15 \\
\widehat{SE}(\hat\beta_1) =& \sqrt{\frac{RSS}{\sum_{i=1}^n(x_i-\xbar)^2}} 
\approx \sqrt{\frac{14.15}{55.7}} \approx 0.504
\end{aligned}
$$ 

... which is the same as r shows us:
```{r, echo=TRUE}
pander::pander(lm0)
```

We saw above, that the estimate for $\sigma=\sqrt{14.15}\approx 3.76$ 
was a bit lower than the actual value of $4$. 
If we look at the 100 different samples, we get 100 estimates that vary around
the true value. Their overall mean is `r mean(sqrt(sig_sq))`.  
That means: $\hat\sigma$ estimates $\sigma$ without bias.


<!-- ```{r} -->
<!-- mean(B1) -->
<!-- sd(B1-3) -->
<!-- qqnorm(B1) -->
<!-- ``` -->

# Hypothesis Testing 

Standard error can be used to conduct *hypothesis tests* on the coefficients. 
Often, one wants to assess wheter there is an relationship between $X$ and $Y$, 
this is the same as testing whether the slope is equal to zero. 

* **$H_0$**:  $\beta_1=0$ i.e. there is no relationship between $X$ and $Y$
* **$H_A$**:  $\beta_1\neq0$ i.e. there is some relationship between $X$ and $Y$

Under the assumption that the null hypothesis is true,
we get a *t-statistic* with $n-2$ degrees of freedom.
$$
t=\frac{\hat\beta_1 - 0}{\widehat{\text{SE}}(\hat\beta_1)} \qquad
$$
This is $t$-distributed since we need to estimate the standard error. 

# Assessing the Accuracy of the Model

**Residual Standard Error (RSE)**

The RSE is an estimate of the standard deviation of the random error $\eps$.

**$R^2$**
$R^2$ expresses the fraction of variance explained in $Y$. 
We set $TSS:=\sum_{i=1}^n(y_i-\ybar)^2$ the total sum of squares.
$$ 
R^2 = \frac{\text{TSS-RSS}{TSS}} = 1-\frac{RSS}{TSS}
$$

In simple linear regression, $R^2$ equals the squared Pearson correlation.

# Multiple Linear Regression

Here we have more than one predictor, so the model is:

$$ 
Y=\beta_0 + \beta_1X_1 + \cdots + \beta_pX_p + \eps
$$ 

We interpret $\beta_j$ as the average eﬀect on $Y$ of a one unit increase in $X_j$,
holding all other predictors ﬁxed.




```{r, webgl=TRUE}
# lm2 <- update(lm1, .~ . + radio) ## add second predictor
# 
# coefs <- coef(lm2)
# a <- coefs["TV"]
# b <- coefs["radio"]
# c <- -1
# d <- coefs["(Intercept)"]
# 
# AA <- BB <- Advertising[,c("TV","radio","sales")]
# BB$sales <- lm2$fitted.values
# CC <- rbind.data.frame(AA,BB)
# CC <- CC[order(CC$TV,CC$radio),]
# 
# plot3d(sales ~ TV + radio, data=Advertising, col=2)
# rglplanes <- planes3d(a, b, c, d, alpha = 0.5, col = 4)

# rglsegments <- segments3d(CC,col=1, lwd=1.5, alpha=0.8)
```

# Small Excursus: Matrix Notation and Hat Matrix

Linear model in matrix notation: 

$$
\underbrace{\left(\begin{smallmatrix} y_1 \\ \vdots \\ y_n \end{smallmatrix}\right)}_{=:Y} =
\underbrace{\left(\begin{smallmatrix} 
1 & x_{11} & \ldots & x_{1p} \\ 
\vdots & \vdots & & \vdots \\ 
1 & x_{n1} & \ldots & x_{np}
\end{smallmatrix}\right)}_{=:X} \cdot 
\underbrace{\left(\begin{smallmatrix}
\beta_0 \\ \vdots \\ \beta_p 
\end{smallmatrix}\right)}_{=:\beta} +
\underbrace{\left(\begin{smallmatrix}
\eps_1 \\ \vdots \\ \eps_n
\end{smallmatrix}\right)}_{\stackrel{i.i.d.}\sim \,\,\, N(0,\sigma^2)}
$$

$X$ is called *design matrix*.  
The estimator for the coefficients can then be expressed as 

$$ 
\hat \beta = \underbrace{(X^T X)^{-1}X^T }_{\small{\text{ Pseudoinverse of }} X} \cdot Y
$$ 

The predictions for $Y$ based on $X$ are then

$$
\hat Y = X\hat \beta = X (X^T X)^{-1}X^T \cdot Y := H \cdot Y
$$

where $H$ is called *prediction* or *hat matrix*.

# Other Considerations



# Example Explained




# Done!

