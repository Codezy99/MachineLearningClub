---
title: >
  ISL - Chapter 10 Labs
author: XXX
output: 
  html_document:
  # BiocStyle::html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: cosmo
    code_folding: show
    code_download: true
editor_options: 
  chunk_output_type: console

---

# Chapter 10 Lab 2: Clustering

# K-Means Clustering

We begin with a simple simulated example in which there truly are two clusters in the data.
```{r}
set.seed(2)
x <- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
group <- c(rep(1,25), rep(2, 25))
plot(x, col= group, pch=20)
```

We now perform K-means clustering with K = 2.
```{r}
km.out <- kmeans(x, 2, nstart = 20)
km.out
km.out$cluster
plot(x, col = (km.out$cluster), main = "K-Means Clustering Results with K=2", xlab = "", ylab = "", pch = 20, cex = 2)
points(x, col=group, pch=1, cex=2)
```
The total within-cluster sum of squares, which we seek to minimize by performing K-means clustering is reported in tot.withinss. The individual within-cluster sum-of-squares are contained in the vector withinss.The explained variance by the cluster is reported by "between_SS / total_SS".


We now perform K-means clustering with K = 3.
```{r}
set.seed(4)
km.out <- kmeans(x, 3, nstart = 20)
km.out
plot(x, col = (km.out$cluster), main = "K-Means Clustering Results with K=3", xlab = "", ylab = "", pch = 20, cex = 2)
points(x, col=group, pch=1, cex=2)
```

To run the kmeans() function in R with multiple initial cluster assignments, we use the nstart argument. If a value of nstart greater than one is used, then K-means clustering will be performed using multiple random assignments in Step 1 of Algorithm 10.1, and the kmeans() function will report only the best results. Here we compare using nstart=1 to nstart=20.  
```{r}
set.seed(7)
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(x, 3, nstart = 20)
km.out$tot.withinss
```
Run K-means clustering always with a large value of nstart, such as 20 or 50, since otherwise an undesirable local optimum may be obtained.



# Hierarchical Clustering

Now we performe hierarchical clustering using complete, single, and average linkage clustering,with Euclidean distance as the dissimilarity measure.
The dist() function is used to compute the 50 × 50 inter-observation Euclidean distance matrix.
```{r}
hc.complete <- hclust(dist(x), method = "complete")
hc.average <- hclust(dist(x), method = "average")
hc.single <- hclust(dist(x), method = "single")

par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage", xlab = "", sub = "", cex = .9)
plot(hc.average, main = "Average Linkage", xlab = "", sub = "", cex = .9)
plot(hc.single, main = "Single Linkage", xlab = "", sub = "", cex = .9)

par(mfrow = c(1, 1))
plot(hc.complete, main = "Complete Linkage", xlab = "", sub = "", cex = .9)


hc.cut <- cutree(hc.complete, 2)
hc.cut
table(hc.cut, group)
plot(x, col=hc.cut, pch = 20, cex = 2)
points(x, col=group, pch=1, cex=2)

hc.cut <- cutree(hc.average, 2)
hc.cut
table(hc.cut, group)
plot(x, col=hc.cut, pch = 20, cex = 2)
points(x, col=group, pch=1, cex=2)
plot(hc.average, main = "Average Linkage", xlab = "", sub = "", cex = .9, labels = group)

hc.cut <- cutree(hc.single, 2)
hc.cut
table(hc.cut, group)

hc.cut <- cutree(hc.single, 4)
hc.cut
table(hc.cut, group)
```

To scale the variables before performing hierarchical clustering of the observations, we use the scale() function:
```{r}
xsc <- scale(x)
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchical Clustering with Scaled Features")
plot(hc.complete, main = "Complete Linkage")
```

Correlation-based distance can be computed using the as.dist() function, which converts an arbitrary square symmetric matrix into a form that the hclust() function recognizes as a distance matrix. 
```{r}

x <- matrix(rnorm(30 * 3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method = "complete"), main = "Complete Linkage with Correlation-Based Distance", xlab = "", sub = "")
```

# Chapter 10 Lab 3: NCI60 Data Example

Unsupervised techniques are often used in the analysis of genomic data. In particular, PCA and hierarchical clustering are popular tools. We illustrate
these techniques on the NCI60 cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines.

# The NCI60 data
```{r}
library("ISLR")
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
```

# Clustering the Observations of the NCI60 Data

We now proceed to hierarchically cluster the cell lines in the NCI60 data, with the goal of finding out whether or not the observations cluster into
distinct types of cancer. To begin, we standardize the variables to have mean zero and standard deviation one. As mentioned earlier, this step is optional and should be performed only if we want each gene to be on the same scale.
We use complete, single, and average linkage. Euclidean distance is used as the dissimilarity measure.

```{r}
sd.data <- scale(nci.data)

data.dist <- dist(sd.data)

par(mfrow = c(1, 3))
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Average Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"), labels = nci.labs, main = "Single Linkage", xlab = "", sub = "", ylab = "")

par(mfrow = c(1, 1))
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Average Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"), labels = nci.labs, main = "Single Linkage", xlab = "", sub = "", ylab = "")


hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out, 4)
table(hc.clusters, nci.labs)
par(mfrow = c(1, 1))
plot(hc.out, labels = nci.labs)
abline(h = 139, col = "red")
hc.out
```

We claimed earlier in Section 10.3.2 that K-means clustering and hierarchical clustering with the dendrogram cut to obtain the same number of clusters can yield very different results. How do these NCI60 hierarchical clustering results compare to what we get if we perform K-means clustering with K = 4?
```{r}
set.seed(2)
km.out <- kmeans(sd.data, 4, nstart = 20)
km.clusters <- km.out$cluster
table(km.clusters, hc.clusters)
```

Rather than performing hierarchical clustering on the entire data matrix, we can simply perform hierarchical clustering on the first few principal component score vectors, as follows:
```{r}
pr.out =prcomp (nci.data , scale=TRUE)
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, labels = nci.labs, main = "Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out, 4), nci.labs)
table(hc.clusters, nci.labs)
```

Sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data. In this situation, we might view the principal component step as one of denoising the data. We could also perform K-means clustering on the first few principal component score vectors rather than the full data set.

# Session Info {-}

```{r}
sessionInfo()
```

    © 2020 GitHub, Inc.
    Terms
    Privacy
    Security
    Status
    Help

    Contact GitHub
    Pricing
    API
    Training
    Blog
    About

